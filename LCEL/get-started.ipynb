{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ユーザが与えたトピックについて短いジョークをLLMに言わせるためのchain\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\") # プロンプトのテンプレート(invoke()を持っていて、このままchainに組み込めるところがふつうのstrと違う)\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\") # モデルにアクセスするためのAPI(モデルとの通信とかもこいつがやってそう)\n",
    "output_parser = StrOutputParser() # モデルから帰ってきた出力をAIMessageからstrにしてくれるやつ、そしてchainに組み込めるやつ\n",
    "\n",
    "chain = prompt | model | output_parser # これでchainを書けるの、イケてると思う\n",
    "\n",
    "# chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prompt\n",
    "\n",
    "プロンプトは`BasePromptTemplate`であり、テンプレート変数の辞書を受け取って`PromptValue`を返す。  \n",
    "`PromptValue`は完成したプロンプトのラッパーで、`LLM`(strを受け取る)や`ChatModel`(メッセージのシーケンスを受け取る)に入力することができる。\n",
    "\n",
    "プロンプトはstring出力と`BaseMessage`のシーケンスのどちらも出力するロジックを持っているので、両方のモデルと組み合わせることができる。\n",
    "\n",
    "めっちゃ大事なこと書いてあった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"}) # `BasePromptTemplate`を`invoke()`すると`PromptValue`が返される\n",
    "prompt_value # PromptValue形式になっていることがわかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages() # `PromptValue`から`BaseMessage`を生成するロジック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string() # `PromptValue`からstrを生成するロジック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model\n",
    "\n",
    "`PromptValue`は`model`に渡される。このケースでは`ChatModel`であり、`BaseMessage`のシーケンスを受け取る。`ChatModel`の出力も`BaseMessage`になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = model.invoke(prompt_value) # PromptValueを受け取ってBaseMessageをかえす\n",
    "# message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelが`LLM`なら出力はstr\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo-instruct\") # `ChatModel`を呼び出すときは`ChatOpenAI()`, `LLM`を呼び出すときは`OpenAI()`\n",
    "# llm.invoke(prompt_value) # チュートリアルと比べて、行頭のRobot:がないので、少し仕様が変わっていそう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Output parser\n",
    "\n",
    "`model`の出力を`output_parser`に渡す。これは`BaseOutputParser`で、stringか`BaseMessage`を入力に受け取る。`StrOutputParser`は特別にどんな入力もstringに変える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_parser.invoke(message) # 今回のは`StrOutputParser`なので、すべての入力はstrになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. パイプライン全体\n",
    "\n",
    "1. ユーザが所望のトピックを`{\"topic\": \"ice cream\"}`として渡す。\n",
    "1. `prompt`がユーザの入力を受け取り、`PromptValue`を出力する\n",
    "1. `model`が`PromptValue`を受け取り、OpenAIのLLM modelに渡して出力を生成してもらう。モデルの出力は`ChatMessage`\n",
    "1. `output_parser`が`ChatMessage`を受け取り、Pythonの文字列に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\"topic\": \"ice cream\"}\n",
    "\n",
    "prompt.invoke(input) # ChatPromptValueを返す\n",
    "# > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (prompt | model).invoke(input) # promptがChatPromptValueを返し、それがmodel(ChatModel, BaseMessageのシーケンスを受け取る)に入力され、AIMessageを返す\n",
    "# > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
