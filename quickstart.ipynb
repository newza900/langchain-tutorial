{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿(OPENAI_API_KEYå¤‰æ•°ã«API KeyãŒå…¥ã£ã¦ã„ã‚‹)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI() # ã“ã‚Œã ã‘ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã§ãã‚‹ã½ã„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='langsmithã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ'\n",
      "content='Langsmithã¯ã€ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã«ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚\\n\\n1. ãƒ†ã‚¹ãƒˆã®ä½œæˆ: Langsmithã¯ã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ç°¡å˜ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€å…¥åŠ›å€¤ã‚„æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ãªã©ã®æƒ…å ±ã‚’æŒ‡å®šã—ã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n2. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: Langsmithã¯ã€ä½œæˆã—ãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã€çµæœã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œçµæœã®ãƒ­ã‚°ã‚„è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚‚æä¾›ã•ã‚Œã¾ã™ã€‚\\n\\n3. ãƒ†ã‚¹ãƒˆã®ç®¡ç†: Langsmithã¯ã€ä½œæˆã—ãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚„å®Ÿè¡Œçµæœã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€ãƒ†ã‚¹ãƒˆã®è¿½åŠ ã€ç·¨é›†ã€å‰Šé™¤ãªã©ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€éå»ã®ãƒ†ã‚¹ãƒˆçµæœã‚’å‚ç…§ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\\n\\n4. ãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ–: Langsmithã¯ã€ãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ–ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€å®šæœŸçš„ãªãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã‚„è‡ªå‹•ãƒ†ã‚¹ãƒˆã®è¨­å®šã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ†ã‚¹ãƒˆã®åŠ¹ç‡åŒ–ã‚„å“è³ªå‘ä¸Šã‚’å›³ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\nä»¥ä¸Šã®ã‚ˆã†ãªæ©Ÿèƒ½ã«ã‚ˆã‚Šã€Langsmithã¯ãƒ†ã‚¹ãƒˆã®ä½œæˆã€å®Ÿè¡Œã€ç®¡ç†ã€è‡ªå‹•åŒ–ãªã©ã‚’æ”¯æ´ã—ã¾ã™ã€‚'\n"
     ]
    }
   ],
   "source": [
    "# ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã¨ä¼šè©±ã™ã‚‹ã“ã¨ãŒã§ãã‚‹\n",
    "print(llm.invoke(\"langsmithã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ã†ã“ã¨ã‚‚ã§ãã‚‹\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ã“ã®ä¾‹ã ã¨ã‚·ã‚¹ãƒ†ãƒ ã«ã€ŒæŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã€ã¨ã„ã†ãƒ­ãƒ¼ãƒ«ã‚’ä¸ãˆã¦ã„ã‚‹\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯ä¸–ç•Œãƒ¬ãƒ™ãƒ«ã®æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ„ã¿åˆã‚ã›ã¦chainã‚’ã¤ãã‚‹\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ã‚ãªãŸã¯ä¸–ç•Œãƒ¬ãƒ™ãƒ«ã®æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚'\n",
      "content='langsmithã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='langsmithã¯ã€ãƒ†ã‚¹ãƒˆæ”¯æ´ã‚’ä»¥ä¸‹ã®ã‚ˆã†ãªæ–¹æ³•ã§è¡Œã„ã¾ã™ï¼š\\n\\n1. ãƒ†ã‚¹ãƒˆè¨ˆç”»ã¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ä½œæˆï¼šlangsmithã¯ã€ãƒ†ã‚¹ãƒˆè¨ˆç”»ã‚„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ä½œæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆè¨ˆç”»ã‚„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æä¾›ã—ã€ãƒ†ã‚¹ãƒˆã®ç›®çš„ã‚„ç¯„å›²ã€å®Ÿæ–½æ‰‹é †ã€æœŸå¾…ã•ã‚Œã‚‹çµæœãªã©ã‚’æ˜ç¢ºã«è¨˜è¿°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n2. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ï¼šãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ç™ºç”Ÿã—ãŸå•é¡Œã‚„ã‚¨ãƒ©ãƒ¼ã€ç‰¹ã«å†ç¾æ€§ã®ä½ã„ãƒã‚°ã«ã¤ã„ã¦ã¯ã€langsmithãŒè©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é–‹ç™ºè€…ãŒå•é¡Œã‚’ä¿®æ­£ã™ã‚‹ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã‚’æä¾›ã—ã€åŠ¹ç‡çš„ãªå•é¡Œè§£æ±ºã‚’æ”¯æ´ã—ã¾ã™ã€‚\\n\\n3. ãƒ†ã‚¹ãƒˆçµæœã®ã¾ã¨ã‚ã¨åˆ†æï¼šãƒ†ã‚¹ãƒˆã®çµæœã‚’ã¾ã¨ã‚ã€å„ªå…ˆé †ä½ä»˜ã‘ã‚„å‚¾å‘ã®åˆ†æã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚langsmithã¯ã€ã‚°ãƒ©ãƒ•ã‚„ãƒãƒ£ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦çµæœã‚’è¦–è¦šåŒ–ã—ã€å“è³ªæ”¹å–„ã®ãŸã‚ã®æ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚\\n\\n4. ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¿å®ˆã¨æ›´æ–°ï¼šlangsmithã¯ã€ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä¿å®ˆã¨æ›´æ–°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚„ãƒ†ã‚¹ãƒˆè¨ˆç”»ã®å¤‰æ›´ãŒã‚ã£ãŸå ´åˆã€langsmithã¯é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è‡ªå‹•çš„ã«æ›´æ–°ã—ã€ä¸€è²«æ€§ã‚’ä¿ã¤ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\nä»¥ä¸Šã®ã‚ˆã†ãªæ©Ÿèƒ½ã«ã‚ˆã‚Šã€langsmithã¯ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã‚’åŠ¹ç‡åŒ–ã—ã€å“è³ªå‘ä¸Šã«è²¢çŒ®ã—ã¾ã™ã€‚')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chainã‚‚invokeã§ä¼šè©±ã§ãã‚‹\n",
    "chain.invoke({\"input\": \"langsmithã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡ºåŠ›ã‚’AIMessageã‹ã‚‰Stringã«å¤‰ãˆã‚‹\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chainã«åŠ ãˆã‚‹\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ã‚ãªãŸã¯ä¸–ç•Œãƒ¬ãƒ™ãƒ«ã®æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚'\n",
      "content='langsmithã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Langsmithã¯ã€ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã«ã•ã¾ã–ã¾ãªæ–¹æ³•ã‚’æä¾›ã—ã¾ã™ã€‚\\n\\n1. ãƒ†ã‚¹ãƒˆè¨ˆç”»ã®ä½œæˆ: Langsmithã¯ã€æ©Ÿèƒ½ã‚„è¦ä»¶ã«åŸºã¥ã„ã¦ãƒ†ã‚¹ãƒˆè¨ˆç”»ã‚’ä½œæˆã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆã®ç¯„å›²ã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€ãƒ†ã‚¹ãƒˆç’°å¢ƒãªã©ã‚’æ–‡æ›¸åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n2. ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ: Langsmithã¯ã€ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãƒ†ã‚¹ãƒˆã®æ‰‹é †ã‚„æœŸå¾…ã•ã‚Œã‚‹çµæœã‚’æ˜ç¢ºã«ç¤ºã™ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\\n\\n3. ãƒ†ã‚¹ãƒˆçµæœã®æ–‡æ›¸åŒ–: Langsmithã¯ã€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®çµæœã‚’æ–‡æ›¸åŒ–ã™ã‚‹ãŸã‚ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚„ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆçµæœã‚„ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’æ­£ç¢ºã«è¨˜éŒ²ã—ã€å•é¡Œã‚’ç‰¹å®šã—è§£æ±ºã™ã‚‹ãŸã‚ã®æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚\\n\\n4. ãƒ†ã‚¹ãƒˆãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®ä½œæˆ: Langsmithã¯ã€ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã‚„ãƒ†ã‚¹ãƒˆæ‰‹æ³•ã«é–¢ã™ã‚‹ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®æ”¯æ´ã‚’è¡Œã„ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆã®å®Ÿæ–½æ‰‹é †ã€ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨æ–¹æ³•ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆæ–¹æ³•ãªã©ã‚’æ–‡æ›¸åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n5. ãƒ†ã‚¹ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ”¯æ´: Langsmithã¯ã€ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ”¯æ´ã—ã¾ã™ã€‚ä»–ã®é–¢ä¿‚è€…ãŒãƒ†ã‚¹ãƒˆè¨ˆç”»ã‚„ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç¢ºèªã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\\n\\nLangsmithã®è‡ªå‹•åŒ–æ©Ÿèƒ½ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ–ã‚„ãƒ†ã‚¹ãƒˆçµæœã®è‡ªå‹•ç”Ÿæˆã‚‚å¯èƒ½ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ†ã‚¹ãƒˆã®åŠ¹ç‡æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"langsmithã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving Deeper\n",
    "\n",
    "ã•ã‚‰ã«æ·±ãç†è§£ã™ã‚‹ãŸã‚ã«ãŸãã•ã‚“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç”¨æ„ã•ã‚Œã¦ã„ã‚‹ã€‚ä»Šã™ãã«ã¯ã¨ã¦ã‚‚èª­ã‚ãªã„ã®ã§ã€å°‘ã—ãšã¤èª­ã‚€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\") # URLã‚’æ¸¡ã™ã¨ãã®å†…å®¹ã‚’å–ã£ã¦ãã¦ãã‚Œã‚‹\n",
    "\n",
    "docs = loader.load() # æŒ‡å®šã—ãŸURLã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«æ ¼ç´ã™ã‚‹ãŸã‚ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# ã“ã‚Œã‚’ä½¿ã£ã¦ã€ä»Šã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æŒ¿å…¥ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å˜ã®ãŸã‚ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ­ãƒ¼ã‚«ãƒ«ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã§ã‚ã‚‹FAISSã‚’ä½¿ã†\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter() # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®åˆ¶é™ã‚’ä¸‹å›ã‚‹ã¾ã§å†å¸°çš„ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã™ã‚‹TextSplitter\n",
    "documents = text_splitter.split_documents(docs) # TextSpritterã«æ›¸ã‘ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã™ã‚‹\n",
    "vector = FAISS.from_documents(documents, embeddings) # åˆ†å‰²ã—ãŸæ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ä¿å­˜ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?\\u200bIf a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?\\u200bBuilding and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debugging\\u200bIn the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a â€œShareâ€ button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examples\\u200bMost of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, itâ€™s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You donâ€™t have any examples to benchmark your changes against.LangSmith addresses this problem by including an â€œAdd to Datasetâ€ button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation\\u200bInitially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "\n",
    "# ä¸ãˆãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã•ã›ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"ä¸ãˆã‚‰ã‚ŒãŸæ–‡è„ˆã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "è³ªå•: {input}\"\"\")\n",
    "\n",
    "# è³ªå•ã¨å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å—ã‘å–ã‚Šã€è§£ç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã™ã‚‹\n",
    "# create_stuff_documents_chainã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ¸¡ã™\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document \n",
    "\n",
    "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è‡ªåˆ†ã§æ¸¡ã™ã“ã¨ã§ã€ãƒã‚§ãƒ¼ãƒ³ã‚’è‡ªåˆ†ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹\n",
    "# document_chain.invoke({\n",
    "#     \"input\": \"how can langsmith help with testing?\",\n",
    "#     \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã§ã‚‚ã€retrieverã‚’ä½¿ã†ã“ã¨ã§ã€è³ªå•ã«æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‹•çš„ã«é¸æŠã—ã¦ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ã“ã¨ãŒã§ãã‚‹\n",
    "from langchain.chains import create_retrieval_chain # retrieval chainã‚’ä½œã‚‹ãŸã‚ã®é–¢æ•°\n",
    "\n",
    "retriever = vector.as_retriever() # retrieverã‚’ä½œæˆ\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain) # retrieverã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å—ã‘å–ã‚‹chainã‹ã‚‰retrieval chainã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã“ã‚Œã§ã€retrieval chainã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã‚‹\n",
    "# response = retrieval_chain.invoke({\"input\": \"LangSmithã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹?\"})\n",
    "# print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving Deeper\n",
    "\n",
    "ã“ã‚Œã‚‚å°‘ã—ãšã¤èª­ã‚€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Retrieval Chain\n",
    "\n",
    "ã“ã“ã¾ã§ã‚„ã£ã¦ããŸã®ã¯1ã¤ã®è³ªå•ã«ç­”ãˆã‚‹chainã€‚ã§ã‚‚ã“ã‚Œã ã¨ä¸€å•ä¸€ç­”ã§ä¼šè©±ãŒã§ããªã„ã€‚ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã«ã‚‚ã“ãŸãˆã‚‰ã‚Œã‚‹chainã‚’ä½œã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã„ã„ã ã‚ã†ã‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_retrieval_chain()ã‚’ã¾ã ä½¿ã†ã“ã¨ãŒã§ãã‚‹ã€‚ã§ã‚‚å¤‰æ›´ã‚’åŠ ãˆã‚‹å¿…è¦ãŒã‚ã‚‹\n",
    "# 1. ç›´è¿‘ã®å…¥åŠ›ã ã‘ã§ãªãã€ã™ã¹ã¦ã®ä¼šè©±ã®æ­´å²ã‚’å…¥åŠ›ã«åŠ ãˆã‚‹\n",
    "# 2. æœ€å¾Œã®LLM chainã‚‚åŒæ§˜ã«ã™ã¹ã¦ã®ä¼šè©±ã®æ­´å²ã‚’å—ã‘å–ã£ã¦å‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever # æ­´å²ã‚’è€ƒæ…®ã—ãŸretriever\n",
    "from langchain_core.prompts import MessagesPlaceholder # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ (ä½•ãªã®ã‹ã‚ˆãã‚ã‹ã‚‰ãªã„ã) \n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã®ä¸­ã«chat_historyã‚’å…¥ã‚Œã‚‹\n",
    "    (\"user\", \"{input}\"), # ãƒ¦ãƒ¼ã‚¶ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆ\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "    # ä»¥ä¸Šã‚’è¸ã¾ãˆã¦ã€ä¼šè©±ã«é–¢ä¿‚ã™ã‚‹æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«æ¤œç´¢ã™ã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt) # æ­´å²ã‚’è¸ã¾ãˆãŸç”Ÿæˆã‚’è¡Œã†chainã‚’ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangSmithã¯ç§ã®LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ'\n",
      "content='ã‚‚ã¡ã‚ã‚“ã§ã™!'\n",
      "content='ã©ã®ã‚ˆã†ã«æ”¯æ´ã—ã¦ãã‚Œã‚‹ã®ã§ã™ã‹'\n",
      "content='Given the above conversation, generate a search query to look up in order to get information relevant to the conversation'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage # äººé–“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨AIã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "\n",
    "# ä»®ã®æ­´å²\n",
    "chat_history = [HumanMessage(content=\"LangSmithã¯ç§ã®LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ\"), AIMessage(content=\"ã‚‚ã¡ã‚ã‚“ã§ã™!\")]\n",
    "\n",
    "# chainã«å…¥åŠ›ã—ã¦ç¶šãã‚’å‡ºåŠ›\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"ã©ã®ã‚ˆã†ã«æ”¯æ´ã—ã¦ãã‚Œã‚‹ã®ã§ã™ã‹\"\n",
    "})\n",
    "\n",
    "# LLMã¯ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€LangSmithã‚’ä½¿ã£ãŸãƒ†ã‚¹ãƒˆã«é–¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿”ã™(ãªã‚“ã§ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ãŸã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¿”ã£ã¦ãã‚‹ã®ã‹ã‚ã‹ã‚“ãªã„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„:\\n\\n{context}\"), # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # ä¼šè©±ã®æ­´å²\n",
    "    (\"user\", \"{input}\"), # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt) # å¤‰æ›´ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å†ã³chainã‚’ç”Ÿæˆ\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain) # æ­´å²ã‚‚å‚ç…§ã§ãã‚‹chainã‚’ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangSmithã¯ç§ã®LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹?'\n",
      "content='Yes!'\n",
      "content='ã©ã®ã‚ˆã†ã«æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ'\n",
      "content='Given the above conversation, generate a search query to look up in order to get information relevant to the conversation'\n",
      "content='ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„:\\n\\nSkip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what\\'s going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what\\n\\nLangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nYou can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We\\'re eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\\n\\ninputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.'\n",
      "content='LangSmithã¯ç§ã®LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹?'\n",
      "content='Yes!'\n",
      "content='ã©ã®ã‚ˆã†ã«æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='LangSmithã¯ç§ã®LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'ã©ã®ã‚ˆã†ã«æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ',\n",
       " 'context': [Document(page_content=\"Skip to main contentğŸ¦œï¸ğŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸ğŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmithã¯ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’åŠ¹æœçš„ã«ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š\\n\\n1. ãƒˆãƒ¬ãƒ¼ã‚¹æ©Ÿèƒ½: LangSmithã¯ã€LLMã®å‘¼ã³å‡ºã—ã‚„ãƒã‚§ãƒ¼ãƒ³ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãªã©ã®ã™ã¹ã¦ã®å‘¼ã³å‡ºã—ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒãƒƒã‚°ã‚„å•é¡Œã®ç‰¹å®šã«å½¹ç«‹ã¡ã¾ã™ã€‚\\n\\n2. ãƒ‡ãƒãƒƒã‚°æ”¯æ´: LLMã€ãƒã‚§ãƒ¼ãƒ³ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‡ãƒãƒƒã‚°ã¯é›£ã—ã„å ´åˆãŒã‚ã‚Šã¾ã™ãŒã€LangSmithã¯ãã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚å…·ä½“çš„ãªå…¥åŠ›ã‚„å‡ºåŠ›ã‚’å¯è¦–åŒ–ã—ã€å•é¡Œã®ç‰¹å®šã‚„ä¿®æ­£ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚\\n\\n3. ãƒ—ãƒ¬ã‚¤ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰: LangSmithã«ã¯ã€LLMã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç·¨é›†ã—ã¦çµæœã‚’è¦³å¯Ÿã™ã‚‹ãŸã‚ã®ãƒ—ãƒ¬ã‚¤ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚ãƒ—ãƒ¬ã‚¤ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç·¨é›†ã—ã€çµæœã®å¤‰åŒ–ã‚’ç¹°ã‚Šè¿”ã—ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n4. ã‚¤ãƒ™ãƒ³ãƒˆã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç¢ºèª: è¤‡é›‘ãªãƒã‚§ãƒ¼ãƒ³ã‚„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ´åˆã€å†…éƒ¨ã§ä½•ãŒèµ·ã“ã£ã¦ã„ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ã®ã¯é›£ã—ã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚LangSmithã¯ã€å‘¼ã³å‡ºã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å¯è¦–åŒ–ã—ã€ç†è§£ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\\n\\n5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚¹ãƒˆ: LangSmithã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã§ãƒã‚§ãƒ¼ãƒ³ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¦–è¦šåŒ–ã—ã¦è©•ä¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n6. äººé–“ã«ã‚ˆã‚‹è©•ä¾¡: LangSmithã§ã¯ã€äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ã‚’å®¹æ˜“ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚æ³¨é‡ˆã‚­ãƒ¥ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã€å®Ÿè¡Œçµæœã‚’äººé–“ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚„è©•ä¾¡ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n\\n7. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°: LangSmithã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ­ã‚°ã‚„ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼çµ±è¨ˆã€ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®å¯è¦–åŒ–ã«ã‚ˆã‚Šã€å•é¡Œã®ç‰¹å®šã‚„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ”¹å–„ã«å½¹ç«‹ã¡ã¾ã™ã€‚\\n\\nã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã«ã‚ˆã‚Šã€LangSmithã¯LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã€å“è³ªã‚„ä¿¡é ¼æ€§ã®å‘ä¸Šã«è²¢çŒ®ã—ã¾ã™ã€‚'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"LangSmithã¯ç§ã®LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"ã©ã®ã‚ˆã†ã«æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "LLMãŒã©ã®ã‚ˆã†ãªæ‰‹é †ã‚’å®Ÿè¡Œã™ã‚‹ã®ã‹ã‚’æ±ºå®šã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã¾ãšã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã©ã®ãƒ„ãƒ¼ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã®ã‹ã‚’å®šç¾©ã™ã‚‹\n",
    "from langchain.tools.retriever import create_retriever_tool # ä»Šã¾ã§ã®ã¯create_retrieval_chainã ã£ãŸ\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever, # ä¼šè©±ã®å±¥æ­´ã‚’å–ã£ã¦ãã‚‹ã‚‚ã®\n",
    "    \"langsmith_search\", # ãƒ„ãƒ¼ãƒ«ã®åå‰?\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\", # ã“ã®ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹ã‚’ãƒ¢ãƒ‡ãƒ«ã«èª¬æ˜ã—ã¦ã‚‹\n",
    "    # â†‘ LangSmithã«é–¢ã™ã‚‹æƒ…å ±ã‚’æ¢ã›ã€‚LangSmithã«é–¢ã™ã‚‹è³ªå•ãŒæ¥ãŸã‚‰ã€ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã‚ãªãã¦ã¯ãªã‚Šã¾ã›ã‚“ã‚ˆï¼\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦Tavilyã‚’ä½¿ã†\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults() # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½¿ã†æ¤œç´¢ãƒ„ãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½¿ã†ãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI # LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "from langchain import hub # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’pullã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "from langchain.agents import create_openai_functions_agent # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "from langchain.agents import AgentExecutor # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\") # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’pullã—ã¦ãã‚‹\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) # ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦APIã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "agent = create_openai_functions_agent(llm, tools, prompt) # ãƒ¢ãƒ‡ãƒ«ã€ãƒ„ãƒ¼ãƒ«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã¾ã¨ã‚ã¦æ¸¡ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹æº–å‚™ã‚’å®Œäº†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_executor.invoke({\"input\": \"what is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result from serializer:{'input': 'LangSmithã£ã¦ä½•ã§ã™ã‹', 'chat_history': [{'content': 'LangSmithã¯LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ', 'type': 'human'}, {'content': 'ã‚‚ã¡ã‚ã‚“ã§ã™ï¼', 'type': 'ai'}]}\n"
     ]
    },
    {
     "ename": "ConnectError",
     "evalue": "[WinError 10061] å¯¾è±¡ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã£ã¦æ‹’å¦ã•ã‚ŒãŸãŸã‚ã€æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_exceptions.py:10\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: PIE786\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_backends\\sync.py:206\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[1;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m--> 206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\socket.py:844\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 844\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\socket.py:832\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 832\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] å¯¾è±¡ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã£ã¦æ‹’å¦ã•ã‚ŒãŸãŸã‚ã€æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_transports\\default.py:67\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_transports\\default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_backends\\sync.py:213\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[1;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[0;32m    212\u001b[0m         sock\u001b[38;5;241m.\u001b[39msetsockopt(\u001b[38;5;241m*\u001b[39moption)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     sock\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mIPPROTO_TCP, socket\u001b[38;5;241m.\u001b[39mTCP_NODELAY, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SyncStream(sock)\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] å¯¾è±¡ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã£ã¦æ‹’å¦ã•ã‚ŒãŸãŸã‚ã€æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# çœŸå˜‰æ¯”ã•ã‚“: chat_historyã¯AI/HumanMessageã®ãƒªã‚¹ãƒˆã§ã‚‚jsonã§ã‚‚ã©ã£ã¡ã§ã‚‚ã„ã„ã€‚Messageã¯jsonã‚’ãƒ©ãƒƒãƒ”ãƒ³ã‚°ã—ã¦ã„ã‚‹ã«éããªã„\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ã“ã®ã‚¨ãƒ©ãƒ¼ã¯æ ¹ãŒæ·±ãã€ã‚½ãƒ¼ã‚¹ã‚’è¦‹ã¦ã‚‚yamachanã•ã‚“ã¨ãƒšã‚¢ãƒ—ãƒ­ã—ã¦ã‚‚è§£æ±ºã§ãã¦ãªã„ãŸã‚ã€ä¸€æ—¦é£›ã°ã™\u001b[39;00m\n\u001b[0;32m      3\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangSmithã¯LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mã‚‚ã¡ã‚ã‚“ã§ã™ï¼\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[1;32m----> 4\u001b[0m \u001b[43mremote_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLangSmithã£ã¦ä½•ã§ã™ã‹\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\langserve\\client.py:300\u001b[0m, in \u001b[0;36mRemoteRunnable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs not implemented yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\langchain_core\\runnables\\base.py:975\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    972\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[0;32m    973\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    974\u001b[0m         Output,\n\u001b[1;32m--> 975\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    976\u001b[0m             call_func_with_variable_args,\n\u001b[0;32m    977\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    978\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    979\u001b[0m             config,\n\u001b[0;32m    980\u001b[0m             run_manager,\n\u001b[0;32m    981\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    982\u001b[0m         ),\n\u001b[0;32m    983\u001b[0m     )\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    985\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\langchain_core\\runnables\\config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    322\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\langserve\\client.py:275\u001b[0m, in \u001b[0;36mRemoteRunnable._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke the runnable with the given input and config.\"\"\"\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult from serializer:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_serializer\u001b[38;5;241m.\u001b[39mdumpd(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 275\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/invoke\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lc_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumpd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_without_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    284\u001b[0m output, callback_events \u001b[38;5;241m=\u001b[39m _decode_response(\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_serializer, response, is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    286\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_client.py:1146\u001b[0m, in \u001b[0;36mClient.post\u001b[1;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1127\u001b[0m     url: URLTypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1139\u001b[0m     extensions: typing\u001b[38;5;241m.\u001b[39mOptional[RequestExtensions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;124;03m    Send a `POST` request.\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_client.py:828\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    813\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m    815\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    816\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    817\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    827\u001b[0m )\n\u001b[1;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    907\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    911\u001b[0m )\n\u001b[0;32m    913\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 915\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    940\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    978\u001b[0m     hook(request)\n\u001b[1;32m--> 980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1013\u001b[0m     )\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1016\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1020\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_transports\\default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    218\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    219\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    220\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    229\u001b[0m )\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    236\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    237\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    238\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    239\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    135\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\masato.arasaki\\source\\repos\\langchain-tutorial\\workspace\\lib\\site-packages\\httpx\\_transports\\default.py:84\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     83\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [WinError 10061] å¯¾è±¡ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã£ã¦æ‹’å¦ã•ã‚ŒãŸãŸã‚ã€æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
     ]
    }
   ],
   "source": [
    "# çœŸå˜‰æ¯”ã•ã‚“: chat_historyã¯AI/HumanMessageã®ãƒªã‚¹ãƒˆã§ã‚‚jsonã§ã‚‚ã©ã£ã¡ã§ã‚‚ã„ã„ã€‚Messageã¯jsonã‚’ãƒ©ãƒƒãƒ”ãƒ³ã‚°ã—ã¦ã„ã‚‹ã«éããªã„\n",
    "# ã“ã®ã‚¨ãƒ©ãƒ¼ã¯æ ¹ãŒæ·±ãã€ã‚½ãƒ¼ã‚¹ã‚’è¦‹ã¦ã‚‚yamachanã•ã‚“ã¨ãƒšã‚¢ãƒ—ãƒ­ã—ã¦ã‚‚è§£æ±ºã§ãã¦ãªã„ãŸã‚ã€ä¸€æ—¦é£›ã°ã™\n",
    "chat_history = [{\"content\": \"LangSmithã¯LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆã‚’æ”¯æ´ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ\", \"type\": \"human\"}, {\"content\": \"ã‚‚ã¡ã‚ã‚“ã§ã™ï¼\", \"type\": \"ai\"}]\n",
    "remote_chain.invoke({\"input\": \"LangSmithã£ã¦ä½•ã§ã™ã‹\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"langsmithã£ã¦ä½•ã§ã™ã‹ï¼Ÿ\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
